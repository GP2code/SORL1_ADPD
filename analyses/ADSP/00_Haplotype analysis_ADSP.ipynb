{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a0b18a-68c4-4524-9d8d-81740b622fd7",
   "metadata": {},
   "source": [
    "# ADSP\n",
    "\n",
    "* **Project:** ADRD-SORL1-Biobanks\n",
    "* **Version:** Python/3.10\n",
    "* **Last Updated:** 20-Aug-2025\n",
    "\n",
    "## Notebook Overview\n",
    "Haplotype analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3901cd",
   "metadata": {},
   "source": [
    "# Query ADSP to perform haplotype analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade9ff0",
   "metadata": {},
   "source": [
    "## Variables used \n",
    "\n",
    "- `${ANCESTRY}` = EUR, AFR, AMR, AAC, AJ, MDE, SAS, CAS, EAS, FIN, CAH\n",
    "- `${COHORT}` = Cases, Controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b4726",
   "metadata": {},
   "source": [
    "### Select SORL1 region ±100 kb boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889022b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e3566-cde7-43b3-a531-38ce0d39a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "plink2 --pfile /${WORK_DIR}/chr11.compact_filtered.r4.wgs.biallelic --chr 11 --from-bp 121352314 --to-bp 121733763 --make-bed --out Haploview_New_Biallelic/Hap_SORL1_boundris "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564bac4c",
   "metadata": {},
   "source": [
    "### Keep biallelic variants and separate the data by ancestry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff12025-d06f-4ba5-884c-cc187512b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "awk 'length($5)==1 && length($6)==1 && $5 ~ /^[ACGT]$/ && $6 ~ /^[ACGT]$/' \\\n",
    "  Haploview_New_Biallelic/Hap_SORL1_boundris.bim | cut -f2 > Haploview_New_Biallelic/biallelic_snps.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61182a8c-19ed-41f6-bdd0-a12f6234b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "plink2 --bfile Haploview_New_Biallelic/Hap_SORL1_boundris \\\n",
    "      --extract Haploview_New_Biallelic/biallelic_snps.txt \\\n",
    "      --make-bed \\\n",
    "      --out Haploview_New_Biallelic/Hap_SORL1_biallelic_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9121d85-769a-4efe-9bde-a806075d1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "input_prefix=\"Haploview_New_Biallelic/Hap_SORL1_biallelic_only\"\n",
    "output_dir=\"Haploview_New_Biallelic\"\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    keep_file=\"adsp_${anc}_keep.txt\"  \n",
    "    output_prefix=\"${output_dir}/Hap_SORL1_biallelic_adsp_${anc}\"\n",
    "\n",
    "    echo \"Processing ancestry: $anc\"\n",
    "    \n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --keep \"$keep_file\" \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_prefix\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c35fdb",
   "metadata": {},
   "source": [
    "### Remove related individuals and perform quality control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad53175-3a0b-46d7-9f8a-0d0ba3965e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "remove_base=\"/${WORK_DIR}\"\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Haploview_New_Biallelic/Hap_SORL1_biallelic_adsp_${anc}\"\n",
    "    remove_file=\"${remove_base}/REMOVE.FILTERED.merged_biallelic_$(echo $anc | tr '[:lower:]' '[:upper:]').related\"\n",
    "    output_prefix=\"Haploview_New_Biallelic/Hap_SORL1_biallelic_adsp_${anc}_unrelated\"\n",
    "\n",
    "    echo \"Processing ${anc}...\"\n",
    "\n",
    "    if [[ -f \"$remove_file\" ]]; then\n",
    "        plink --bfile \"$input_prefix\" \\\n",
    "              --remove \"$remove_file\" \\\n",
    "              --make-bed \\\n",
    "              --out \"$output_prefix\"\n",
    "    else\n",
    "        echo \"Warning: Remove file not found for ${anc}: ${remove_file}\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8ab3c-81cd-4107-8236-f45cfd4cfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_file=\"Haploview_New_Biallelic/Hap_SORL1_biallelic_adsp_${anc}_unrelated\"\n",
    "    output_file=\"Haploview_New_Biallelic/Hap_SORL1_biallelic_adsp_${anc}_unrelated_filtered\"\n",
    "\n",
    "    echo \"Processing ${anc} with MAF and geno filters...\"\n",
    "\n",
    "    plink --bfile \"$input_file\" \\\n",
    "          --maf 0.05 \\\n",
    "          --geno 0.05 \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_file\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99ac31",
   "metadata": {},
   "source": [
    "### Convert the data to VCF format to prepare for phasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8eb93b-4649-4f7c-8ee0-f8f889cb1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Haploview_New_Biallelic/Hap_SORL1_biallelic_adsp_${anc}_unrelated_filtered\"\n",
    "    output_prefix=\"Haploview_New_Biallelic/${anc}_biallelic_SORL1\"\n",
    "\n",
    "    echo \"Converting ${anc} PLINK to VCF (with IID sample IDs)...\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --recode vcf-iid \\\n",
    "          --out \"$output_prefix\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e51373",
   "metadata": {},
   "source": [
    "### Phasing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf04a7-2c97-4aae-bcec-137e231a6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://faculty.washington.edu/browning/beagle/beagle.28Jun21.220.jar -O beagle.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feedf36-8360-4dde-85c0-12f75be4a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! module load java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19197ea8-4967-4564-9a99-c98b08117aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "java -Xmx8g -jar beagle.jar \\\n",
    "  gt=Haploview_New_Biallelic/${ANCESTRY}_biallelic_SORL1.vcf \\\n",
    "  out=Haploview_New_Biallelic/${ANCESTRY}_biallelic_SORL1_phased \\\n",
    "  nthreads=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240ffd4",
   "metadata": {},
   "source": [
    "### Covert data to PLINK format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa4bf2-015c-4867-b36a-13b6c756fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9  \n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_vcf=\"Haploview_New_Biallelic/${anc}_biallelic_SORL1_phased.vcf.gz\"\n",
    "    output_prefix=\"Haploview_New_Biallelic/${anc}_biallelic_SORL1_phased\"\n",
    "\n",
    "    echo \"Converting phased VCF of ${anc} to PLINK...\"\n",
    "\n",
    "    plink --vcf \"$input_vcf\" \\\n",
    "          --double-id \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_prefix\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bdfdb",
   "metadata": {},
   "source": [
    "### Add phenotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf5c45-00fd-486f-a0b9-d57a9bd90133",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo -e \"FID\\tIID\\tPHENO\" > haplo_phenotypes.txt\n",
    "\n",
    "awk 'NR>1 {print $1, $2, 2}' qc_case_plink.txt >> haplo_phenotypes.txt\n",
    "\n",
    "awk 'NR>1 {print $1, $2, 1}' qc_control_plink.txt >> haplo_phenotypes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a486e2-9ba6-426c-9df1-fb212327603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -v \"FID\" haplo_phenotypes.txt | awk '$3 == 1' | wc -l\n",
    "grep -v \"FID\" haplo_phenotypes.txt | awk '$3 == 2' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d8a51-13d2-48d3-8b63-38c7ba87f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9  \n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Haploview_New_Biallelic/${anc}_biallelic_SORL1_phased\"\n",
    "    output_prefix=\"Haploview_New_Biallelic/${anc}_biallelic_SORL1_phased_with_pheno\"\n",
    "\n",
    "    echo \"Adding phenotype to ${anc}...\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --pheno <(tail -n +2 haplo_phenotypes.txt) \\\n",
    "          --allow-no-sex \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_prefix\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79127bcc",
   "metadata": {},
   "source": [
    "### Create necessary files (.ped and .info) for Haploview input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea750ce-ca5e-4acd-a0cd-8cdba651b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Haploview_New_Biallelic/${anc}_biallelic_SORL1_phased_with_pheno\"\n",
    "    output_prefix=\"Haploview_New_Biallelic/${anc}_biallelic_SORL1_haploview\"\n",
    "\n",
    "    echo \"Generating Haploview input files for ${anc}...\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --recodeHV \\\n",
    "          --allow-no-sex \\\n",
    "          --out \"$output_prefix\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40e56e",
   "metadata": {},
   "source": [
    "### Run the analysis in a GUI environment in Haploview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861233c-e689-4af7-aacf-ca88ebd4cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://www.broadinstitute.org/ftp/pub/mpg/haploview/Haploview.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f354c",
   "metadata": {},
   "source": [
    "### Clean the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64eaac3b-99fb-48e2-8875-34f11a7bd5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ancestries = ['eur', 'afr', 'amr', 'eas', 'sas', 'aac', 'aj', 'cah', 'cas', 'mde', 'fin']\n",
    "output_dir = \"Haploview_New_Biallelic\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for anc in ancestries:\n",
    "    file_path = f\"Haploview_New_Biallelic/{anc}_results1\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        continue\n",
    "    \n",
    "    data = []\n",
    "    current_block = None  \n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        header_line = f.readline().strip()\n",
    "        header_cols = header_line.split('\\t')\n",
    "        if header_cols[0].lower() == \"block\":\n",
    "            header_cols = header_cols[1:]  \n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"Block \"):  \n",
    "                current_block = line  \n",
    "                continue\n",
    "            fields = line.split('\\t')\n",
    "            if len(fields) == len(header_cols):\n",
    "                \n",
    "                data.append([current_block] + fields)\n",
    "    \n",
    "    \n",
    "    df_columns = ['Block'] + [col.strip().replace(' ', '_').replace('.', '').replace(',', '') for col in header_cols]\n",
    "    df = pd.DataFrame(data, columns=df_columns)\n",
    "    \n",
    "    \n",
    "    numeric_cols = ['Freq', 'Chi_Square', 'P_Value']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    \n",
    "    if 'CaseControl_Frequencies' in df.columns:\n",
    "        df[['CaseFreq', 'ControlFreq']] = df['CaseControl_Frequencies'].str.strip().str.split(',', expand=True).astype(float)\n",
    "    \n",
    "    \n",
    "    cols_to_drop = ['Case_Control_Ratio_Counts', 'CaseControl_Frequencies']\n",
    "    for col in cols_to_drop:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    \n",
    "    cols_to_keep = ['Block', 'Haplotype', 'Freq', 'Chi_Square', 'P_Value', 'CaseFreq', 'ControlFreq']\n",
    "    df = df[cols_to_keep]\n",
    "    \n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{anc}_cleaned.tsv\")\n",
    "    df.to_csv(output_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09acbc6b",
   "metadata": {},
   "source": [
    "### Seperate significant results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5f6f3c-ef07-4b81-a43a-2a62baab6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ancestries = ['eur', 'afr', 'amr', 'eas', 'sas', 'aac', 'aj', 'cah', 'cas', 'mde', 'fin']\n",
    "input_dir = \"Haploview_New_Biallelic\"\n",
    "\n",
    "for anc in ancestries:\n",
    "    input_file = os.path.join(input_dir, f\"{anc}_cleaned.tsv\")\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        continue\n",
    "    \n",
    "   \n",
    "    df = pd.read_csv(input_file, sep='\\t')\n",
    "    \n",
    "    \n",
    "    df['Direction'] = df.apply(\n",
    "        lambda row: '↑ in cases' if row['CaseFreq'] > row['ControlFreq']\n",
    "        else '↑ in controls' if row['CaseFreq'] < row['ControlFreq']\n",
    "        else 'no difference',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    sig_df = df[df['P_Value'] < 0.05].copy()\n",
    "    sig_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    output_file = os.path.join(input_dir, f\"{anc}_significant.tsv\")\n",
    "    sig_df.to_csv(output_file, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ece9b",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b738cd-11c1-4ce5-8133-93bed06dcec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "ancestries = ['eur', 'afr', 'amr', 'aac', 'aj', 'cas', 'eas', 'sas', 'mde', 'cah']\n",
    "input_dir = \"Haploview_New_Biallelic\"\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "pdf_output = os.path.join(input_dir, \"all_log2fc_heatmaps.pdf\")\n",
    "\n",
    "with PdfPages(pdf_output) as pdf:\n",
    "    for anc in ancestries:\n",
    "        input_file = os.path.join(input_dir, f\"{anc}_significant.tsv\")\n",
    "        \n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"File not found: {input_file}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(input_file, sep='\\t')\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"No significant results for {anc.upper()}\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        df['FoldChange'] = df['CaseFreq'] / df['ControlFreq']\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df['log2FC'] = np.log2(df['FoldChange'])\n",
    "\n",
    "        df = df.sort_values(by='FoldChange', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        output_table = os.path.join(input_dir, f\"{anc}_significant_with_foldchange.tsv\")\n",
    "        df.to_csv(output_table, sep='\\t', index=False)\n",
    "        print(f\"Saved TSV: {output_table}\")\n",
    "\n",
    "        \n",
    "        heatmap_df = df.set_index('Haplotype')[['log2FC']].copy()\n",
    "        finite_vals = heatmap_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        \n",
    "        if finite_vals.dropna().empty and not any(np.isinf(heatmap_df['log2FC'])):\n",
    "            print(f\"Skipped heatmap for {anc.upper()} — nothing to plot\")\n",
    "            continue\n",
    "\n",
    "        mask = heatmap_df['log2FC'].isin([np.inf, -np.inf]).values.reshape(-1, 1)\n",
    "\n",
    "        \n",
    "        max_hap_len = max(len(h) for h in heatmap_df.index)\n",
    "        fig_width = 10 if max_hap_len < 40 else 12 if max_hap_len < 80 else 14\n",
    "        fig_height = max(3.5, len(heatmap_df) * 0.45)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "        sns.heatmap(\n",
    "            finite_vals,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            cbar_kws={'label': 'log2(CaseFreq / ControlFreq)', 'shrink': 0.6},\n",
    "            mask=mask,\n",
    "            annot_kws={\"color\": \"black\"},\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        for i, (hap, val) in enumerate(heatmap_df['log2FC'].items()):\n",
    "            if val == np.inf or val == -np.inf:\n",
    "                color = 'red' if val == np.inf else 'blue'\n",
    "                ax.add_patch(plt.Rectangle((0, i), 1, 1, fill=True, color=color))\n",
    "                ax.text(0.5, i + 0.5, '∞', va='center', ha='center', color='black', fontsize=10, fontweight='bold')\n",
    "\n",
    "       \n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=6)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), fontsize=6)\n",
    "        ax.set_title(f'Log2 Fold Change (Case vs Control): {anc.upper()}', fontsize=10)\n",
    "        ax.set_ylabel('Haplotype', fontsize=8)\n",
    "        ax.set_xlabel(\"\")  \n",
    "\n",
    "       \n",
    "        colorbar = ax.collections[0].colorbar\n",
    "        colorbar.ax.tick_params(labelsize=6)\n",
    "        colorbar.set_label('log2(CaseFreq / ControlFreq)', fontsize=7)\n",
    "\n",
    "        \n",
    "        plt.subplots_adjust(left=0.42, right=0.92, top=0.88, bottom=0.08)\n",
    "\n",
    "       \n",
    "        heatmap_image = os.path.join(input_dir, f\"{anc}_log2fc_heatmap.png\")\n",
    "        plt.savefig(heatmap_image, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved PNG: {heatmap_image}\")\n",
    "\n",
    "        \n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Added to PDF: {anc.upper()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78301634-edc9-4cfb-8332-5e0dca4fd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "input_dir = \"Haploview_New_Biallelic\"\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "ordered_ancestries = ['eur', 'afr', 'amr', 'aac', 'aj', 'cas', 'eas', 'sas', 'mde', 'cah']\n",
    "\n",
    "pdf_output = os.path.join(input_dir, \"all_case_enriched_heatmaps.pdf\")\n",
    "\n",
    "with PdfPages(pdf_output) as pdf:\n",
    "    for anc in ordered_ancestries:\n",
    "        input_file = os.path.join(input_dir, f\"{anc}_significant.tsv\")\n",
    "\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"File not found: {input_file}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No significant results for {anc.upper()}\")\n",
    "            continue\n",
    "\n",
    "        df['FoldChange'] = df['CaseFreq'] / df['ControlFreq']\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df['log2FC'] = np.log2(df['FoldChange'])\n",
    "\n",
    "        df_case_enriched = df[df['log2FC'] > 0].copy()\n",
    "\n",
    "        if df_case_enriched.empty:\n",
    "            print(f\"No case-enriched haplotypes for {anc.upper()}\")\n",
    "            continue\n",
    "\n",
    "        df_case_enriched = df_case_enriched.sort_values(by='log2FC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        tsv_file = os.path.join(input_dir, f\"{anc}_case_enriched.tsv\")\n",
    "        df_case_enriched.to_csv(tsv_file, sep='\\t', index=False)\n",
    "        print(f\"Saved TSV: {tsv_file}\")\n",
    "\n",
    "        heatmap_df = df_case_enriched.set_index('Haplotype')[['log2FC']].copy()\n",
    "        finite_vals = heatmap_df.replace([np.inf], np.nan)\n",
    "        mask = heatmap_df['log2FC'].isin([np.inf]).values.reshape(-1, 1)\n",
    "\n",
    "        max_hap_len = max(len(h) for h in heatmap_df.index)\n",
    "        fig_width = 10 if max_hap_len < 40 else 12 if max_hap_len < 80 else 14\n",
    "        fig_height = max(2, len(heatmap_df) * 0.5)\n",
    "\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        ax = sns.heatmap(\n",
    "            finite_vals,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap='Reds',\n",
    "            cbar_kws={'label': 'log2(CaseFreq / ControlFreq)'},\n",
    "            mask=mask,\n",
    "            annot_kws={\"color\": \"black\"}\n",
    "        )\n",
    "\n",
    "        for i, (hap, val) in enumerate(heatmap_df['log2FC'].items()):\n",
    "            if val == np.inf:\n",
    "                plt.gca().add_patch(plt.Rectangle((0, i), 1, 1, fill=True, color='darkred'))\n",
    "                plt.text(0.5, i + 0.5, '+inf', va='center', ha='center', color='black')\n",
    "\n",
    "        plt.yticks(rotation=0, fontsize=7)\n",
    "        plt.title(f'Case-Enriched Haplotypes: {anc.upper()}')\n",
    "        plt.ylabel('Haplotype')\n",
    "\n",
    "        if len(heatmap_df) < 4:\n",
    "            ax.figure.axes[-1].yaxis.label.set_size(8)\n",
    "            plt.xlabel('', fontsize=8)\n",
    "            plt.xticks(fontsize=8)\n",
    "        else:\n",
    "            plt.xlabel('')\n",
    "            plt.xticks(fontsize=10)\n",
    "\n",
    "        plt.subplots_adjust(left=0.45, right=0.95, top=0.85, bottom=0.10)\n",
    "\n",
    "        png_file = os.path.join(input_dir, f\"{anc}_case_enriched_heatmap.png\")\n",
    "        plt.savefig(png_file)\n",
    "        print(f\"Saved PNG: {png_file}\")\n",
    "\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        print(f\"Added to PDF: {anc.upper()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33403f4f",
   "metadata": {},
   "source": [
    "# Creating Haplotype Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6593c",
   "metadata": {},
   "source": [
    "### Select SORL1 region ±100 kb boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8825cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "plink2 --pfile /${WORK_DIR}/chr11.compact_filtered.r4.wgs.biallelic --chr 11 --from-bp 121352314 --to-bp 121733763 --make-bed --out Blocks/Hap_SORL1_boundris "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b236770",
   "metadata": {},
   "source": [
    "### Separate the data by ancestry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "input_prefix=\"Blocks/Hap_SORL1_boundris\"\n",
    "output_dir=\"Blocks\"\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    keep_file=\"adsp_${anc}_keep.txt\"  \n",
    "    output_prefix=\"${output_dir}/Hap_SORL1_boundris_adsp_${anc}\"\n",
    "\n",
    "    echo \"Processing ancestry: $anc\"\n",
    "    \n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --keep \"$keep_file\" \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_prefix\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43797185",
   "metadata": {},
   "source": [
    "### Remove related individuals and perform quality control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "remove_base=\"/${WORK_DIR}/plink_files_genotools_update_2024\"\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Blocks/Hap_SORL1_boundris_adsp_${anc}\"\n",
    "    remove_file=\"${remove_base}/REMOVE.FILTERED.merged_biallelic_$(echo $anc | tr '[:lower:]' '[:upper:]').related\"\n",
    "    output_prefix=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated\"\n",
    "\n",
    "    echo \"Processing ${anc}...\"\n",
    "\n",
    "    if [[ -f \"$remove_file\" ]]; then\n",
    "        plink --bfile \"$input_prefix\" \\\n",
    "              --remove \"$remove_file\" \\\n",
    "              --make-bed \\\n",
    "              --out \"$output_prefix\"\n",
    "    else\n",
    "        echo \"Warning: Remove file not found for ${anc}: ${remove_file}\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87cd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_file=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated\"\n",
    "    output_file=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered\"\n",
    "\n",
    "    echo \"Processing ${anc} with MAF and geno filters...\"\n",
    "\n",
    "    plink --bfile \"$input_file\" \\\n",
    "          --maf 0.05 \\\n",
    "          --geno 0.05 \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_file\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6038da",
   "metadata": {},
   "source": [
    "### Separate the data by cases and controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered\"\n",
    "    output_dir=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered_case\"\n",
    "    keep_file=\"qc_case_plink.txt\"  \n",
    "    output_prefix=\"${output_dir}/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered_case\"\n",
    "\n",
    "    echo \"Processing ancestry: $anc\"\n",
    "    \n",
    "    mkdir -p \"$output_dir\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --keep \"$keep_file\" \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_prefix\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230606e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered\"\n",
    "    output_dir=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered_control\"\n",
    "    keep_file=\"qc_control_plink.txt\"  \n",
    "    output_prefix=\"${output_dir}/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered_control\"\n",
    "\n",
    "    echo \"Processing ancestry: $anc\"\n",
    "    \n",
    "    mkdir -p \"$output_dir\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --keep \"$keep_file\" \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_prefix\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba9710",
   "metadata": {},
   "source": [
    "### Convert the data to VCF format to prepare for phasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d13ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "# --- Controls ---\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered_control/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered_control\"\n",
    "    output_prefix=\"Blocks/${anc}_control\"\n",
    "\n",
    "    echo \"Converting ${anc} CONTROL PLINK to VCF (with IID sample IDs)...\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --recode vcf-iid \\\n",
    "          --out \"$output_prefix\"\n",
    "done\n",
    "\n",
    "# --- Cases ---\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Blocks/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered_case/Hap_SORL1_boundris_adsp_${anc}_unrelated_filtered_case\"\n",
    "    output_prefix=\"Blocks/${anc}_case\"\n",
    "\n",
    "    echo \"Converting ${anc} CASE PLINK to VCF (with IID sample IDs)...\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --recode vcf-iid \\\n",
    "          --out \"$output_prefix\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca6b63",
   "metadata": {},
   "source": [
    "### Phasing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "! module load java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "java -Xmx8g -jar beagle.jar \\\n",
    "  gt=Blocks/${ANCESTRY}_${COHORT}.vcf \\\n",
    "  out=Blocks/${ANCESTRY}_${COHORT}_phased \\\n",
    "  nthreads=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50773c79",
   "metadata": {},
   "source": [
    "### Covert data to PLINK format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cf9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9  \n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "# --- Controls ---\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_vcf=\"Blocks/${anc}_control_phased.vcf.gz\"\n",
    "    output_prefix=\"Blocks/${anc}_control_phased\"\n",
    "\n",
    "    echo \"Converting phased VCF of ${anc} (controls) to PLINK...\"\n",
    "\n",
    "    plink --vcf \"$input_vcf\" \\\n",
    "          --double-id \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_prefix\"\n",
    "done\n",
    "\n",
    "# --- Cases ---\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_vcf=\"Blocks/${anc}_case_phased.vcf.gz\"\n",
    "    output_prefix=\"Blocks/${anc}_case_phased\"\n",
    "\n",
    "    echo \"Converting phased VCF of ${anc} (cases) to PLINK...\"\n",
    "\n",
    "    plink --vcf \"$input_vcf\" \\\n",
    "          --double-id \\\n",
    "          --make-bed \\\n",
    "          --out \"$output_prefix\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b6720",
   "metadata": {},
   "source": [
    "### Generation of Haplotype Blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9750073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Blocks/${anc}_case_phased\"\n",
    "    output_prefix=\"Blocks/${anc}_case_phased_blocks\"\n",
    "\n",
    "    echo \"Processing ancestry: $anc\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --blocks no-pheno-req \\\n",
    "          --out \"$output_prefix\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7089de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "ancestries=(eur afr amr eas sas aac aj cah cas mde fin)\n",
    "\n",
    "for anc in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Blocks/${anc}_control_phased\"\n",
    "    output_prefix=\"Blocks/${anc}_control_phased_blocks\"\n",
    "\n",
    "    echo \"Processing ancestry: $anc\"\n",
    "\n",
    "    plink --bfile \"$input_prefix\" \\\n",
    "          --blocks no-pheno-req \\\n",
    "          --out \"$output_prefix\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb127b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.10",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
