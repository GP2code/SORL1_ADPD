{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91d2407",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "# ADSP\n",
    "\n",
    "* **Project:** ADRD-SORL1-Biobanks\n",
    "* **Version:** Python/3.10\n",
    "* **Last Updated:** 14-Jun-2025\n",
    "\n",
    "## Notebook Overview\n",
    "Characterization of SORL1 variants, allele freqs, association analysis, burden analysis, clinical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c15dd",
   "metadata": {},
   "source": [
    "# Query ADSP to check for variants of interest, allele frequency, and to calculate missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf597d",
   "metadata": {},
   "source": [
    "## Variables used \n",
    "\n",
    "- `${ANCESTRY}` = EUR, AFR, AMR, AAC, AJ, MDE, SAS, CAS, EAS, FIN, CAH\n",
    "- `chr${}:Position:A1:A2` = Chromosome number, position, reference and alternative alleles\n",
    "- `${}= 1-10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaf304e2-6cfe-4a98-b211-ea94aa0a3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "\n",
    "plink2 \\\n",
    "  --pfile /${WORK_DIR}/chr11.compact_filtered.r4.wgs.biallelic \\\n",
    "  --chr 11 \\\n",
    "  --from-bp 121452314 \\\n",
    "  --to-bp 121633763 \\\n",
    "  --mac 2 \\\n",
    "  --make-bed \\\n",
    "  --out bed_Alex_output.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "\n",
    "plink2 \\\n",
    "  --pfile /${WORK_DIR}/chr11.compact_filtered.r4.wgs.biallelic \\\n",
    "  --chr 11 \\\n",
    "  --from-bp 121452314 \\\n",
    "  --to-bp 121633763 \\\n",
    "  --mac 2 \\\n",
    "  --make-pgen \\\n",
    "  --out Alex_output.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "\n",
    "plink2 --bfile bed_Alex_output.txt --recode A --out recoded_bed_Alex_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a77b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "plink2 -pfile Alex_output.txt --missing --out Alex_output.txt_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "plink2 --pfile Alex_output.txt --freq --out Alex_output.txt_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "${ANCESTRY}= pd.read_csv(\"${WORK_DIR}/FILTERED.merged_biallelic_${ANCESTRY}.psam\", sep = '\\t')\n",
    "${ANCESTRY}.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "${ANCESTRY}_keep = ${ANCESTRY} [[\"#FID\", \"IID\"]]\n",
    "${ANCESTRY}_keep.to_csv(\"adsp_${ANCESTRY}_keep.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3606729",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "plink --bfile bed_Alex_output.txt --keep adsp_${ANCESTRY}_keep.txt --make-bed --out bed_Alex_adsp_${ANCESTRY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9  \n",
    "plink --bfile bed_Alex_adsp_${ANCESTRY} --remove /${WORK_DIR}/REMOVE.FILTERED.merged_biallelic_${ANCESTRY}.related --make-bed --out bed_Alex_adsp_${ANCESTRY}_unrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "plink --bfile bed_Alex_adsp_${ANCESTRY}_unrelated --freq --out bed_Alex_adsp_${ANCESTRY}_unrelated_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9231c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_covar = pd.read_csv(\"/${WORK_DIR}/covars_for_QC.txt\", sep=\"\\t\")\n",
    "qc_covar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_case = qc_covar[qc_covar[\"PHENO\"]==2]\n",
    "qc_case.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_case_plink = qc_case[[\"FID\", \"IID\"]]\n",
    "qc_case_plink.to_csv(\"qc_case_plink.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_control_plink = qc_control[[\"FID\", \"IID\"]]\n",
    "qc_control_plink.to_csv(\"qc_control_plink.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad00e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "plink --bfile bed_Alex_adsp_${ANCESTRY}_unrelated --keep qc_case_plink.txt --make-bed --out bed_Alex_adsp_${ANCESTRY}_unrelated_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cec7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "plink --bfile bed_Alex_adsp_${ANCESTRY}_unrelated_cases --freq --out bed_Alex_adsp_${ANCESTRY}_unrelated_cases_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eef0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "plink --bfile bed_Alex_adsp_${ANCESTRY}_unrelated --keep qc_control_plink.txt --make-bed --out bed_Alex_adsp_${ANCESTRY}_unrelated_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "plink --bfile bed_Alex_adsp_${ANCESTRY}_unrelated_controls --freq --out bed_Alex_adsp_${ANCESTRY}_unrelated_controls_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6929cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "\n",
    "plink2 --bfile  bed_Alex_adsp_${ANCESTRY}_unrelated_cases --recode A --out recoded__${ANCESTRY}_case_Alex_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56bedc",
   "metadata": {},
   "source": [
    "# Association analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from functools import reduce\n",
    "import argparse\n",
    "from glob import glob\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6befffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "plink2 --bfile bed_Alex_output.txt --recode vcf-iid --out Alex_output_MAC2_recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c023a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load annovar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "table_annovar.pl Alex_output_MAC2_recode.vcf $ANNOVAR_DATA/hg38 \\\n",
    "    --buildver hg38 \\\n",
    "    --remove \\\n",
    "    --thread 48 \\\n",
    "    --maxgenethread 48 \\\n",
    "    --protocol refGene,clinvar_20140902,avsnp151,gnomad_genome,dbnsfp47a \\\n",
    "    --operation g,f,f,f,f \\\n",
    "    --nopolish \\\n",
    "    --nastring . \\\n",
    "    --out Alex_output_MAC2_recode.vcf.annovar \\\n",
    "    --vcfinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file into a DataFrame\n",
    "input_file = \"Alex_output_MAC2_recode.vcf.annovar.hg38_multianno.txt\"\n",
    "output_file = \"Alex_exonic_splicing_variants.txt\"\n",
    "\n",
    "# Read the tab-delimited file\n",
    "df = pd.read_csv(input_file, sep=\"\\t\")\n",
    "\n",
    "# Filter for exonic and splicing variants\n",
    "filtered_df = df[df[\"Func.refGene\"].isin([\"exonic\", \"splicing\"])]\n",
    "\n",
    "# Save the result to a new file\n",
    "filtered_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Filtered variants saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Full ancestry list\n",
    "ancestries = [\"EUR\", \"AFR\", \"AMR\", \"EAS\", \"SAS\", \"AAC\", \"MDE\", \"AJ\", \"FIN\", \"CAS\", \"CAH\"]\n",
    "\n",
    "# Generate keep files\n",
    "for ancestry in ancestries:\n",
    "    pheno_file = f\"/${WORK_DIR}/FID_IID_PHENO_{ancestry}.fam\"\n",
    "    try:\n",
    "        df = pd.read_csv(pheno_file, delim_whitespace=True)\n",
    "        keep_file = f\"{ancestry}.keep\"\n",
    "        df[['FID', 'IID']].to_csv(keep_file, sep=' ', index=False, header=False)\n",
    "        print(f\"Created {keep_file} from {pheno_file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"WARNING: {pheno_file} not found, skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "base_plink_file=\"bed_Alex_output.txt\"\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "    keep_file=\"${ancestry}.keep\"\n",
    "    subset_prefix=\"temp_${ancestry}\"\n",
    "    final_prefix=\"Alex_output_MAC2_${ancestry}\"\n",
    "\n",
    "    if [[ -f \"$keep_file\" ]]; then\n",
    "        echo \"Processing $ancestry...\"\n",
    "\n",
    "        # Step 1: Subset samples by ancestry\n",
    "        plink \\\n",
    "            --bfile \"$base_plink_file\" \\\n",
    "            --keep \"$keep_file\" \\\n",
    "            --make-bed \\\n",
    "            --out \"$subset_prefix\"\n",
    "\n",
    "        # Step 2: Filter variants with at least 1 minor allele in subset\n",
    "        plink \\\n",
    "            --bfile \"$subset_prefix\" \\\n",
    "            --mac 1 \\\n",
    "            --make-bed \\\n",
    "            --out \"$final_prefix\"\n",
    "\n",
    "        # Remove temporary files\n",
    "        rm \"${subset_prefix}\".bed \"${subset_prefix}\".bim \"${subset_prefix}\".fam\n",
    "\n",
    "    else\n",
    "        echo \"WARNING: $keep_file not found, skipping $ancestry.\"\n",
    "    fi\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ee0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "\n",
    "# List of 11 ancestries\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "# Loop through each ancestry and run plink2 recode\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "    input_prefix=\"Alex_output_MAC2_${ancestry}\"\n",
    "    output_prefix=\"${input_prefix}_recode\"\n",
    "\n",
    "    if [[ -f \"${input_prefix}.bed\" ]]; then\n",
    "        echo \"Running plink2 recode for ${ancestry}...\"\n",
    "        plink2 \\\n",
    "            --bfile \"$input_prefix\" \\\n",
    "            --recode vcf-iid \\\n",
    "            --out \"$output_prefix\"\n",
    "    else\n",
    "        echo \"WARNING: ${input_prefix}.bed not found. Skipping ${ancestry}.\"\n",
    "    fi\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List of 11 ancestries\n",
    "ancestries = [\"EUR\", \"AFR\", \"AMR\", \"EAS\", \"SAS\", \"AAC\", \"MDE\", \"AJ\", \"FIN\", \"CAS\", \"CAH\"]\n",
    "\n",
    "# Input exonic variant list\n",
    "exonic_variants_file = \"Alex_exonic_splicing_variants.txt\"\n",
    "\n",
    "# Load exonic variants\n",
    "exonic_variants = pd.read_csv(exonic_variants_file, sep=\"\\t\")\n",
    "exonic_variants[\"Chr\"] = exonic_variants[\"Chr\"].astype(str)\n",
    "exonic_variants[\"Start\"] = exonic_variants[\"Start\"].astype(str)\n",
    "\n",
    "# Process each ancestry's VCF\n",
    "for ancestry in ancestries:\n",
    "    vcf_file = f\"Alex_output_MAC2_{ancestry}_recode.vcf\"\n",
    "    output_vcf = f\"Alex_exonic_splicing_output_{ancestry}.vcf\"\n",
    "\n",
    "    if not os.path.exists(vcf_file):\n",
    "        print(f\"VCF for {ancestry} not found: {vcf_file}\")\n",
    "        continue\n",
    "\n",
    "    # Read VCF\n",
    "    vcf_header = []\n",
    "    vcf_data = []\n",
    "    with open(vcf_file, \"r\") as vcf:\n",
    "        for line in vcf:\n",
    "            if line.startswith(\"#\"):\n",
    "                vcf_header.append(line)\n",
    "            else:\n",
    "                vcf_data.append(line.strip().split(\"\\t\"))\n",
    "\n",
    "    if not vcf_data:\n",
    "        print(f\"No variant data found in {vcf_file}\")\n",
    "        continue\n",
    "\n",
    "    vcf_columns = vcf_header[-1].strip().split(\"\\t\")\n",
    "    vcf_df = pd.DataFrame(vcf_data, columns=vcf_columns)\n",
    "\n",
    "    # Filter VCF\n",
    "    filtered_vcf = vcf_df[\n",
    "        (vcf_df[\"#CHROM\"].isin(exonic_variants[\"Chr\"])) &\n",
    "        (vcf_df[\"POS\"].isin(exonic_variants[\"Start\"])) &\n",
    "        (vcf_df[\"REF\"].isin(exonic_variants[\"Ref\"])) &\n",
    "        (vcf_df[\"ALT\"].isin(exonic_variants[\"Alt\"]))\n",
    "    ]\n",
    "\n",
    "    # Write filtered VCF\n",
    "    with open(output_vcf, \"w\") as out_vcf:\n",
    "        out_vcf.writelines(vcf_header)\n",
    "        for _, row in filtered_vcf.iterrows():\n",
    "            out_vcf.write(\"\\t\".join(row) + \"\\n\")\n",
    "\n",
    "    print(f\"[âœ“] Filtered VCF written for {ancestry}: {output_vcf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List of ancestries\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "# Loop through and gzip each file\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "    vcf_file=\"Alex_exonic_splicing_output_${ancestry}.vcf\"\n",
    "    if [[ -f \"$vcf_file\" ]]; then\n",
    "        echo \"Compressing $vcf_file...\"\n",
    "        gzip \"$vcf_file\"\n",
    "    else\n",
    "        echo \"WARNING: $vcf_file not found, skipping.\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec37e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/2.0\n",
    "\n",
    "# Define an array of ancestries\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "# Define the output directory\n",
    "output_dir=\"/${WORK_DIR}\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "mkdir -p \"$output_dir\"\n",
    "\n",
    "# Loop over each ancestry\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "  echo \"Processing ancestry: $ancestry\"\n",
    "\n",
    "  # Define VCF file inside the loop so $ancestry is expanded\n",
    "  vcf_file=\"/${WORK_DIR}/Alex_exonic_splicing_output_${ancestry}.vcf.gz\"\n",
    "\n",
    "  # Run PLINK\n",
    "  plink2 \\\n",
    "    --vcf \"$vcf_file\" \\\n",
    "    --double-id \\\n",
    "    --pheno \"/${WORK_DIR}/FID_IID_PHENO_${ancestry}.fam\" \\\n",
    "    --adjust \\\n",
    "    --ci 0.95 \\\n",
    "    --covar \"/${WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA.txt\" \\\n",
    "    --covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10 \\\n",
    "    --threads 15 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out \"${output_dir}/New_Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr11_exonic_splicing_Alex\" \\\n",
    "    --glm omit-ref firth-fallback cols=+a1freq,+a1freqcc,+a1count,+totallele,+a1countcc,+totallelecc,+gcountcc,+err \\\n",
    "    --silent\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = \"/${WORK_DIR}\"\n",
    "\n",
    "# Define the list of files and their corresponding ancestry codes\n",
    "files_and_ancestries = [\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_AAC_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"AAC\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_AFR_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"AFR\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_AMR_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"AMR\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_AJ_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"AJ\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_EUR_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"EUR\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_CAS_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"CAS\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_SAS_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"SAS\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_MDE_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"MDE\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_EAS_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"EAS\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_FIN_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"FIN\"),\n",
    "    (\"New_Logistic_FID_IID_PHENO_case_controls_CAH_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logistic.hybrid\", \"CAH\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for file_name, ancestry in files_and_ancestries:\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Add the new column with the corresponding ancestry code\n",
    "    df['ancestry'] = ancestry\n",
    "\n",
    "    # Save the updated DataFrame to a new file\n",
    "    output_file_name = file_name.replace(\".hybrid\", \"New__with_ancestry.txt\")\n",
    "    output_file_path = os.path.join(dir_path, output_file_name)\n",
    "    df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "    print(f\"Updated file saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5153d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = \"/${WORK_DIR}\"\n",
    "\n",
    "# List of file paths to be combined\n",
    "file_paths = [\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_AAC_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_AFR_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_AMR_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_AJ_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_EUR_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_CAS_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_SAS_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_MDE_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_EAS_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_FIN_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "    \"New_Logistic_FID_IID_PHENO_case_controls_CAH_vars_chr11_exonic_splicing_Alex.PHENO1.glm.logisticNew__with_ancestry.txt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to hold the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Read each file and append the DataFrame to the list\n",
    "for file_name in file_paths:\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new file\n",
    "output_file_path = os.path.join(dir_path, \"New_Combined_Logistic_chr11_exonic_splicing_Alex_with_ancestry.hybrid\")\n",
    "combined_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Combined data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e752a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined data file safely\n",
    "file_path = \"/${WORK_DIR}/New_Combined_Logistic_chr11_exonic_splicing_Alex_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# Filter for rows where TEST == \"ADD\"\n",
    "filtered_df = df[df['TEST'] == 'ADD']\n",
    "\n",
    "# Save the filtered DataFrame\n",
    "output_file_path = \"/${WORK_DIR}/New_Filtered_Combined_Logistic_chr11_exonic_splicing_Alex_with_ancestry.hybrid\"\n",
    "filtered_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered data file\n",
    "file_path = \"/${WORK_DIR}/New_Filtered_Combined_Logistic_chr11_exonic_splicing_Alex_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_keep = ['#CHROM', 'POS', 'REF', 'ALT', 'A1', 'P', 'OR', 'L95', 'U95', 'ancestry']\n",
    "filtered_df = df[columns_to_keep]\n",
    "\n",
    "# Save the filtered DataFrame to a new file\n",
    "output_file_path = \"/${WORK_DIR}/New_Selected_Combined_Logistic_chr11_exonic_splicing_Alex_with_ancestry.hybrid\"\n",
    "filtered_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Data with selected columns saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ff76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data file\n",
    "file_path = \"/${WORK_DIR}/New_Selected_Combined_Logistic_chr11_exonic_splicing_Alex_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Define the desired order of ancestries\n",
    "ancestry_order = ['EUR', 'AFR', 'AMR', 'EAS', 'SAS', 'MDE', 'AJ', 'FIN', 'AAC', 'CAS', 'CAH']\n",
    "\n",
    "# Initialize a list to hold the rows for the new table\n",
    "combined_data = []\n",
    "\n",
    "# Group the data by variation (defined by #CHROM, POS, REF, ALT)\n",
    "grouped = df.groupby(['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "\n",
    "# Iterate over each group and format the data for the combined table\n",
    "for (chrom, pos, ref, alt), group in grouped:\n",
    "    # Append the variation row\n",
    "    combined_data.append([f\"{chrom}:{pos} {ref}>{alt}\", \"\", \"\", \"\"])\n",
    "    \n",
    "    # Append the header row for ancestries\n",
    "    combined_data.append([\"Ancestry\", \"A1\", \"P\", \"OR (L95_U95)\"])\n",
    "    \n",
    "    # Create a dictionary of ancestries and their data\n",
    "    ancestry_dict = {row['ancestry']: [row['A1'], row['P'], f\"{row['OR']} ({row['L95']}_{row['U95']})\"] for _, row in group.iterrows()}\n",
    "    \n",
    "    # Append the data rows for each ancestry in the specified order if it exists\n",
    "    for ancestry in ancestry_order:\n",
    "        if ancestry in ancestry_dict:\n",
    "            combined_data.append([ancestry] + ancestry_dict[ancestry])\n",
    "\n",
    "    # Add an empty row for separation between variations\n",
    "    combined_data.append([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "# Convert the combined data into a DataFrame\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# Save the combined DataFrame to a new file\n",
    "output_file_path = \"New_Combined_Variations_Table_chr11_exonic_splicing_Alex.tsv\"\n",
    "combined_df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "print(f\"Combined variations table saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e335e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "# Define an array of ancestries\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "# Define the output directory\n",
    "output_dir=\"/{WORK_DIR}\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "mkdir -p \"$output_dir\"\n",
    "\n",
    "# Loop over each ancestry\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "  echo \"Processing ancestry: $ancestry\"\n",
    "\n",
    "  # Define VCF file inside the loop\n",
    "  vcf_file=\"${output_dir}/Alex_exonic_splicing_output_${ancestry}.vcf.gz\"\n",
    "\n",
    "  if [[ -f \"$vcf_file\" ]]; then\n",
    "    plink \\\n",
    "      --vcf \"$vcf_file\" \\\n",
    "      --double-id \\\n",
    "      --pheno \"/{WORK_DIR}/logistic_regression/FID_IID_PHENO_${ancestry}.fam\" \\\n",
    "      --pheno-name PHENO1 \\\n",
    "      --assoc \\\n",
    "      --ci 0.95 \\\n",
    "      --adjust \\\n",
    "      --allow-no-sex \\\n",
    "      --threads 15 \\\n",
    "      --out \"${output_dir}/New_Assoc_FID_IID_PHENO_case_controls_${ancestry}_chr11_exonic_splicing_Alex\" \\\n",
    "      --silent\n",
    "  else\n",
    "    echo \"WARNING: VCF file for $ancestry not found: $vcf_file\"\n",
    "  fi\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7909f99",
   "metadata": {},
   "source": [
    "# Burden analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbaeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://github.com/zhanxw/rvtests/releases/download/v2.1.0/rvtests_linux64.tar.gz\n",
    "! tar -xvzf  rvtests_linux64.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Define an array of ancestries\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "# Directory containing the VCF files\n",
    "vcf_dir=\"/${WORK_DIR}\"\n",
    "\n",
    "# Loop through each ancestry\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "    vcf_file=\"${vcf_dir}/Alex_exonic_splicing_output_${ancestry}.vcf\"\n",
    "    \n",
    "    if [[ -f \"$vcf_file\" ]]; then\n",
    "        echo \"Compressing $vcf_file...\"\n",
    "        bgzip -k \"$vcf_file\"\n",
    "    else\n",
    "        echo \"WARNING: File $vcf_file not found. Skipping.\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Define an array of ancestries\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "# Directory containing the VCF files\n",
    "vcf_dir=\"/${WORK_DIR}\"\n",
    "\n",
    "# Loop through each ancestry\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "    vcf_file=\"${vcf_dir}/Alex_exonic_splicing_output_${ancestry}.vcf.gz\"\n",
    "    \n",
    "    if [[ -f \"$vcf_file\" ]]; then\n",
    "        echo \"Indexing $vcf_file...\"\n",
    "        tabix -f -p vcf \"$vcf_file\"\n",
    "    else\n",
    "        echo \"WARNING: File $vcf_file not found. Skipping.\"\n",
    "    fi\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b279a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covar = pd.read_csv(\"/${WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA.txt\", sep=\"\\t\")\n",
    "df_covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestry in ['EUR', 'AFR', 'AMR', 'EAS', 'SAS', 'MDE', 'AJ', 'FIN', 'AAC', 'CAS', 'CAH']:\n",
    "    df_pheno = pd.read_csv(f\"/${WORK_DIR}/logistic_regression/FID_IID_PHENO_{ancestry}.fam\", sep=\" \")\n",
    "    df_covar_ancestry = df_covar.merge(df_pheno, on=[\"FID\",\"IID\"], how=\"inner\")\n",
    "    df_covar_ancestry.to_csv(f\"/${WORK_DIR}/burden_covar_{ancestry}.txt\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestry in ['EUR', 'AFR', 'AMR', 'EAS', 'SAS', 'MDE', 'AJ', 'FIN', 'AAC', 'CAS', 'CAH']:\n",
    "    !executable/rvtest \\\n",
    "    --inVcf \"/${WORK_DIR}/Alex_exonic_splicing_output_{ancestry}.vcf.gz\" \\\n",
    "    --out \"/${WORK_DIR}/NewAlex_exonic_splicing_SORL1_Burden_{ancestry}\" \\\n",
    "    --numThread 10 \\\n",
    "    --noweb \\\n",
    "    --hide-covar \\\n",
    "    --kernel skat,skato \\\n",
    "    --pheno \"/${WORK_DIR}/burden_covar_{ancestry}.txt\" \\\n",
    "    --pheno-name PHENO \\\n",
    "    --geneFile \"/${WORK_DIR}/refFlat.txt\" \\\n",
    "    --covar \"/${WORK_DIR}/burden_covar_{ancestry}.txt\" \\\n",
    "    --covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10 \\\n",
    "    --multipleAllele \\\n",
    "    --gene SORL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!executable/rvtest \\\n",
    "--inVcf \"/${WORK_DIR}/Alex_exonic_splicing_output.vcf.gz\" \\\n",
    "--out \"Alex-MDE-BURDEN\" \\\n",
    "--pheno \"/d${WORK_DIR}/burden_covar_MDE.txt\" \\\n",
    "--pheno-name PHENO \\\n",
    "--covar \"/${WORK_DIR}/burden_covar_MDE.txt\" \\\n",
    "--covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10 \\\n",
    "--multipleAllele \\\n",
    "--single wald,score \\\n",
    "--numThread 10 \\\n",
    "--noweb \\\n",
    "--hide-covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8263666",
   "metadata": {},
   "outputs": [],
   "source": [
    "! awk '$13 < 0.05' Alex-MDE-BURDEN.SingleScore.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk 'NR > 1 && $13 < 0.05' Alex-MDE-BURDEN.SingleScore.assoc | sort -k13,13g | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a288b6e",
   "metadata": {},
   "source": [
    "# Clinical data for 10 identified variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "plink2 -pfile /${WORK_DIR}/chr11.compact_filtered.r4.wgs.biallelic --snps chr${}:Position:A1:A2   --make-bed --out chr11-${}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de39f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink\n",
    "plink2 --bfile chr11-${}  --recode A --out chr11-${}_recoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '$7 == 0 || $7 == 1' chr11-${}_recoded.raw > chr11-${}_recoded.raw.filtered.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat chr11-${}_recoded.raw.filtered.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load plink/1.9\n",
    "\n",
    "plink --bfile chr11-${} --keep qc_case_plink.txt --make-bed --out chr11-${}_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "fam_file = 'chr11-${}_cases.fam'\n",
    "fam_df = pd.read_csv(fam_file, delim_whitespace=True, header=None, usecols=[0, 1], names=['FID', 'IID'])\n",
    "\n",
    "raw_file = 'chr11-${}_recoded.raw.filtered.raw'\n",
    "raw_df = pd.read_csv(raw_file, delim_whitespace=True, header=None, usecols=[0, 1], names=['FID', 'IID'])\n",
    "\n",
    "merged_df = pd.merge(fam_df, raw_df, on=['FID', 'IID'], how='inner')\n",
    "\n",
    "merged_df.to_csv('chr11-${}_cases_filtered.fam', sep=' ', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c267379",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat chr11-${}_cases_filtered.fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e12a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Alex_case_sampleids2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a208edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "qc_covar = pd.read_csv(\"/${WORK_DIR}/covars_for_QC.txt\", sep=\"\\t\")\n",
    "\n",
    "case_sampleids = pd.read_csv(\"Alex_case_sampleids2.txt\", delim_whitespace=True, header=None, names=['FID', 'IID'])\n",
    "\n",
    "merged_df = pd.merge(qc_covar, case_sampleids, on=['FID', 'IID'], how='inner')\n",
    "\n",
    "merged_df.to_csv(\"qc_covar_Alex_case_sampleids.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53beb8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clinical_data = pd.read_csv(\"/${WORK_DIR}/ADNIPhenotypes_DS_2022.08.18_ALL.txt\", sep=\"\\t\")\n",
    "\n",
    "case_sampleids = pd.read_csv(\"Alex_case_sampleids2.txt\", delim_whitespace=True, header=None, names=['FID', 'IID'])\n",
    "\n",
    "first_column_name = clinical_data.columns[0]\n",
    "\n",
    "filtered_clinical_data = clinical_data[clinical_data[first_column_name].isin(case_sampleids['FID'])]\n",
    "\n",
    "filtered_clinical_data.to_csv(\"Alex_clinical_case_sampleids.txt\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Alex_clinical_case_sampleids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aaba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clinical_data = pd.read_csv(\"/${WORK_DIR}/ADSPCaseControlPhenotypes_DS_2022.08.18_ALL.txt\", sep=\"\\t\", low_memory=False)\n",
    "\n",
    "case_sampleids = pd.read_csv(\"Alex_case_sampleids2.txt\", delim_whitespace=True, header=None, names=['FID', 'IID'])\n",
    "\n",
    "first_column_name = clinical_data.columns[0]\n",
    "\n",
    "filtered_clinical_data = clinical_data[clinical_data[first_column_name].isin(case_sampleids['FID'])]\n",
    "\n",
    "filtered_clinical_data.to_csv(\"Alex_clinical2_case_sampleids.txt\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Alex_clinical2_case_sampleids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a41fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clinical_data = pd.read_csv(\"/${WORK_DIR}/ADSPFamilyBasedPhenotypes_DS_2022.08.18_ALL.txt\", sep=\"\\t\", low_memory=False, encoding='latin1')\n",
    "\n",
    "case_sampleids = pd.read_csv(\"Alex_case_sampleids2.txt\", delim_whitespace=True, header=None, names=['FID', 'IID'])\n",
    "\n",
    "first_column_name = clinical_data.columns[0]\n",
    "\n",
    "filtered_clinical_data = clinical_data[clinical_data[first_column_name].isin(case_sampleids['FID'])]\n",
    "\n",
    "filtered_clinical_data.to_csv(\"Alex_clinical3_case_sampleids.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52659d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Alex_clinical3_case_sampleids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clinical_data = pd.read_csv(\"/${WORK_DIR}/ADSP-PHC-Biomarker_DS_2022.09.27_ALL.csv\", sep=\",\", low_memory=False, encoding='latin1')\n",
    "\n",
    "case_sampleids = pd.read_csv(\"Alex_case_sampleids2.txt\", delim_whitespace=True, header=None, names=['FID'])\n",
    "\n",
    "filtered_clinical_data = clinical_data[clinical_data['SUBJID'].isin(case_sampleids['FID'])]\n",
    "\n",
    "filtered_clinical_data.to_csv(\"Alex_clinical4_case_sampleids.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0932566",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Alex_clinical4_case_sampleids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a40e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clinical_data = pd.read_csv(\"/${WORK_DIR}/phenotype_data/ADSP-PHC-Cognition_DS_2022.09.27_ALL.csv\", sep=\",\", low_memory=False, encoding='latin1')\n",
    "\n",
    "case_sampleids = pd.read_csv(\"Alex_case_sampleids2.txt\", delim_whitespace=True, header=None, names=['FID'])\n",
    "\n",
    "filtered_clinical_data = clinical_data[clinical_data['SUBJID'].isin(case_sampleids['FID'])]\n",
    "\n",
    "filtered_clinical_data.to_csv(\"Alex_clinical5_case_sampleids.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Alex_clinical5_case_sampleids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clinical_data = pd.read_csv(\"/${WORK_DIR}/ADSP-PHC-Neuropath_DS_2022.09.27_ALL.csv\", sep=\",\", low_memory=False, encoding='latin1')\n",
    "\n",
    "case_sampleids = pd.read_csv(\"Alex_case_sampleids2.txt\", delim_whitespace=True, header=None, names=['FID'])\n",
    "\n",
    "filtered_clinical_data = clinical_data[clinical_data['SUBJID'].isin(case_sampleids['FID'])]\n",
    "\n",
    "filtered_clinical_data.to_csv(\"Alex_clinical6_case_sampleids.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Alex_clinical6_case_sampleids.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.10",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
