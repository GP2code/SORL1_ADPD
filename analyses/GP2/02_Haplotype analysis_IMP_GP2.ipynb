{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ad39f7-2757-4e48-b906-b74f64993e21",
   "metadata": {},
   "source": [
    "# GP2 R10 Imputed Data\n",
    "\n",
    "* **Project:** ADRD-SORL1-Biobanks\n",
    "* **Version:** Python/3.10\n",
    "* **Last Updated:** 21-Aug-2025\n",
    "\n",
    "## Notebook Overview\n",
    "Haplotype analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84e1c1-c708-499b-a155-f1935fe196b2",
   "metadata": {},
   "source": [
    "### Variable setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dfdb8f-b8e9-4645-bf0b-24cd7c46c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry = \"EUR\"\n",
    "ancestry_list = [\"AAC\", \"AFR\", \"AJ\", \"AMR\",\"CAH\", \"CAS\", \"EUR\", \"EAS\", \"FIN\", \"MDE\", \"SAS\"]\n",
    "CHROM = 'chr11'\n",
    "GENE = 'SORL1'\n",
    "WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "! mkdir -p {WORK_DIR}\n",
    "DATA_DIR = f'/home/jupyter/workspace'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9231cb-32b4-4356-89ff-4192d91087ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b158a1df-8c25-4fc8-91dc-8d5bf881bd8a",
   "metadata": {},
   "source": [
    "### Make covariate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4765075-68bb-4041-aa70-195e962f725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_stats = []\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! mkdir -p {WORK_DIR}\n",
    "    \n",
    "    # Let's load the master key\n",
    "    key = pd.read_csv(f'{DATA_DIR}/gp2_tier2_eu_release10/clinical_data/master_key_release10_final_vwb.csv', header=0)\n",
    "    print(key.shape)\n",
    "    key = key[['GP2ID', 'baseline_GP2_phenotype', 'baseline_GP2_phenotype_for_qc', 'biological_sex_for_qc', 'age_at_sample_collection', 'age_of_onset',  \"age_at_diagnosis\",'race_for_qc','nba', 'wgs', 'nba_label', 'study_type']]\n",
    "    key.rename(columns = {'GP2ID': 'IID',\n",
    "                          'baseline_GP2_phenotype':'phenotype',\n",
    "                                         'biological_sex_for_qc':'SEX', \n",
    "                                         'age_at_sample_collection':'AGE', \n",
    "                                         'race_for_qc':'RACE',\n",
    "                                         'age_of_onset':'AAO',\n",
    "                                     \"age_at_diagnosis\":\"AAD\"}, inplace = True)\n",
    "    \n",
    "    ## Subset to keep ancestry of interest \n",
    "    ancestry_key = key[key['nba_label']==ancestry].copy()\n",
    "    ancestry_key.reset_index(drop=True)\n",
    "    \n",
    "    # Load information about related individuals \n",
    "    try:\n",
    "        related_df = pd.read_csv(f'{DATA_DIR}/gp2_tier2_eu_release10/meta_data/related_samples/{ancestry}_release10_vwb.related')\n",
    "        print(related_df.shape)\n",
    "        # Make a list of just one set of related people\n",
    "        related_list = list(related_df['IID1'])\n",
    "    \n",
    "        # remove related individuals\n",
    "        print(f\"Removing {len(related_list)} individuals from {ancestry}.\")\n",
    "    \n",
    "        ancestry_key = ancestry_key[~ancestry_key[\"IID\"].isin(related_list)]\n",
    "\n",
    "\n",
    "        ancestry_key\n",
    "    except:\n",
    "        print(\"Warning. No related samples file found. Removing 0 samples.\")\n",
    "\n",
    "    \n",
    "    pheno_mapping = {\"PD\": 2, \"Control\": 1}\n",
    "    ancestry_key['PHENO'] = ancestry_key['phenotype'].map(pheno_mapping).astype('Int64')\n",
    "    \n",
    "    ## Get the PCs\n",
    "    pcs = pd.read_csv(f'{DATA_DIR}/gp2_tier2_eu_release10/raw_genotypes/{ancestry}/{ancestry}_release10_vwb.eigenvec', sep='\\t')\n",
    "    selected_columns = ['IID', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n",
    "    pcs = pd.DataFrame(data=pcs.iloc[:, 1:7].values, columns=selected_columns)\n",
    "    \n",
    "    # Drop the first row (since it's now the column names)\n",
    "    pcs = pcs.drop(0)\n",
    "    \n",
    "    # Reset the index to remove any potential issues\n",
    "    pcs = pcs.reset_index(drop=True)\n",
    "    \n",
    "    # Convert phenotype to binary (1/2)\n",
    "    ## Assign conditions so female=2 and men=1, and -9 otherwise (matching PLINK convention)\n",
    "    # Female = 2; Male = 1\n",
    "    sex_mapping = {\"Female\": 2, \"Male\": 1}\n",
    "    ancestry_key[\"SEX\"] = ancestry_key['SEX'].map(sex_mapping).astype('Int64')\n",
    "    \n",
    "    # Check value counts of SEX\n",
    "    ancestry_key[\"SEX\"].value_counts(dropna=False)\n",
    "    \n",
    "    print(f\"Dropping {(~ancestry_key['nba']==1).sum()} samples that are not genotyped\")\n",
    "    print(f\"Dropping {(ancestry_key['wgs']==1).sum()} samples that have whole genome sequencing available\")\n",
    "    ancestry_key = ancestry_key[(ancestry_key[\"nba\"]==1) & (ancestry_key[\"wgs\"]==0) ]\n",
    "    \n",
    "    ## Make covariate file\n",
    "    df = pd.merge(pcs, ancestry_key, on='IID', how=\"inner\")\n",
    "    \n",
    "    print(f\"Dropping {ancestry_key.shape[0] - df.shape[0]} samples with no pcs\")\n",
    "    ## Drop lines with missing pheno\n",
    "    print(f\"Dropping {(df['PHENO'].isna()).sum()} samples with missing pheno\")\n",
    "    \n",
    "    df = df[df['PHENO'].notna()]\n",
    "    \n",
    "    df[\"PHENO\"].value_counts(dropna=False)\n",
    "    \n",
    "    df[\"#FID\"] = 0\n",
    "    final_df = df[['#FID','IID', 'SEX', 'AGE', 'PHENO', 'AAO', 'AAD', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']].copy()\n",
    "    final_df.groupby(['PHENO'])['SEX'].value_counts()\n",
    "    \n",
    "    ## Make file of sample IDs to keep \n",
    "    samples_toKeep = final_df[['#FID', 'IID']].copy()\n",
    "    \n",
    "    samples_toKeep.to_csv(f'{WORK_DIR}/{ancestry}.samplestoKeep', sep = '\\t', index=False, header=None)\n",
    "    \n",
    "    ## Save your covariate file\n",
    "    final_df.to_csv(f'{WORK_DIR}/{ancestry}_covariate_file.txt', sep = '\\t', index=False, na_rep= \"NA\", header=True)\n",
    "    \n",
    "    cohort_stats.append((ancestry,(final_df[\"PHENO\"] == 2).sum(), (final_df[\"PHENO\"] == 1).sum()))\n",
    "cohort_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f710bff-714c-4045-b0c8-470a140d77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to tab-delimited file\n",
    "with open(f'/home/jupyter/{GENE}_results/haplotype_files/case_control_cts_imputed.txt', 'w') as f:\n",
    "    f.write(\"Ancestry\\tCases\\tControls\\n\")\n",
    "    for row in cohort_stats:\n",
    "        f.write(f\"{row[0]}\\t{row[1]}\\t{row[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4541b-fca2-4445-b156-851ef9344046",
   "metadata": {},
   "source": [
    "### Make Phenotype Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cc85ee-418d-45ee-9cb9-94ef83fbb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! cut -f 1,2,5 {WORK_DIR}/{ancestry}_covariate_file.txt > {WORK_DIR}/{ancestry}_pheno.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408667b-2f5b-4a4f-bbf3-21712dbe5bc4",
   "metadata": {},
   "source": [
    "### Select SORL1 region ±100 kb boundaries, remove related individuals and restrict to case and controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8df91510-1b52-43bb-8a45-a4ff5618031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for all other ancestries\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! mkdir -p {WORK_DIR}\n",
    "    ! /home/jupyter/tools/plink2 \\\n",
    "    --pfile {DATA_DIR}/gp2_tier2_eu_release10/imputed_genotypes/{ancestry}/{CHROM}_{ancestry}_release10_vwb \\\n",
    "    --chr 11 \\\n",
    "    --from-bp 121352314 \\\n",
    "    --to-bp 121733763 \\\n",
    "    --keep {WORK_DIR}/{ancestry}.samplestoKeep \\\n",
    "    --pheno {WORK_DIR}/{ancestry}_pheno.txt \\\n",
    "    --pheno-name PHENO \\\n",
    "    --make-bed \\\n",
    "    --silent \\\n",
    "    --out {WORK_DIR}/{ancestry}_boundaries_Hap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852318ba-6ba5-47a0-bc5d-209916a04134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only biallelic variants\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! awk 'length($5)==1 && length($6)==1 && $5 ~ /^[ACGT]$/ && $6 ~ /^[ACGT]$/' \\\n",
    "   {WORK_DIR}/{ancestry}_boundaries_Hap.bim | cut -f2 > {WORK_DIR}/{ancestry}_biallelic_snps.txt\n",
    "    ! /home/jupyter/tools/plink2 --bfile {WORK_DIR}/{ancestry}_boundaries_Hap \\\n",
    "      --extract {WORK_DIR}/{ancestry}_biallelic_snps.txt \\\n",
    "      --make-bed \\\n",
    "      --out {WORK_DIR}/{ancestry}_biallelic_Hap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102411db-3676-418f-84bd-a49f9b9361e5",
   "metadata": {},
   "source": [
    "### Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879cd2f-aee9-4700-89af-265cb22bbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make covariate files\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! /home/jupyter/tools/plink2 \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_biallelic_Hap \\\n",
    "    --maf 0.05 \\\n",
    "    --geno 0.05 \\\n",
    "    --make-bed \\\n",
    "    --out {WORK_DIR}/{ancestry}_biallelic_Hap_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28fd30-54e0-4c7a-930c-ddabb461d035",
   "metadata": {},
   "source": [
    "### Create VCF files for phasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc27b021-9d98-4374-ab14-928a0d975675",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_biallelic_Hap_filtered \\\n",
    "    --silent \\\n",
    "    --recode vcf-iid \\\n",
    "    --out {WORK_DIR}/{ancestry}_biallelic_Hap_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5225d-da35-42d2-b8c8-fd989b33e592",
   "metadata": {},
   "source": [
    "### Phasing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97ec6e-fd75-4af8-82e1-3443104d6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download \n",
    "! wget -O /home/jupyter/{GENE}_results/haplotype_files/beagle.jar https://faculty.washington.edu/browning/beagle/beagle.28Jun21.220.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a0762-320e-4c97-bd11-3ad24151d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! java -Xmx8g -jar /home/jupyter/{GENE}_results/haplotype_files/beagle.jar \\\n",
    "      gt={WORK_DIR}/{ancestry}_biallelic_Hap_filtered.vcf \\\n",
    "      out={WORK_DIR}/{ancestry}_biallelic_Hap_filtered_phased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c207c7-51dd-4472-9d95-75af974fb329",
   "metadata": {},
   "source": [
    "### Create new pheno file with double-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe6715bf-4fd4-49c8-862a-de00f7879171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need pheno file to match new fam file\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    pheno = pd.read_csv(f'{WORK_DIR}/{ancestry}_pheno.txt', sep=\"\\t\")\n",
    "    pheno[\"#FID\"] = pheno[\"IID\"]\n",
    "    pheno.to_csv(f'{WORK_DIR}/{ancestry}_pheno_doubleid.txt', sep=\"\\t\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967a910-c7cb-4860-981d-2b8145ef18f3",
   "metadata": {},
   "source": [
    "### Convert phased file to PLINK 1.9 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9254ed53-a4e3-4c6b-9ffe-9de096de5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --vcf {WORK_DIR}/{ancestry}_biallelic_Hap_filtered_phased.vcf.gz \\\n",
    "    --double-id \\\n",
    "    --pheno {WORK_DIR}/{ancestry}_pheno_doubleid.txt \\\n",
    "    --pheno-name PHENO \\\n",
    "    --allow-no-sex \\\n",
    "    --silent \\\n",
    "    --make-bed \\\n",
    "    --out {WORK_DIR}/{ancestry}_biallelic_Hap_filtered_phased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b072c87-9894-4324-aba6-b08cb08e7361",
   "metadata": {},
   "source": [
    "### Create necessary files (.ped and .info) for Haploview input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0eef83e-6904-4504-bf69-e91e4a34a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_biallelic_Hap_filtered_phased \\\n",
    "    --recodeHV \\\n",
    "    --silent \\\n",
    "    --allow-no-sex \\\n",
    "    --out {WORK_DIR}/{ancestry}_biallelic_{GENE}_haploview_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906cfd2",
   "metadata": {},
   "source": [
    "### Run the analysis in a GUI environment in Haploview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://www.broadinstitute.org/ftp/pub/mpg/haploview/Haploview.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f4d32",
   "metadata": {},
   "source": [
    "### Clean the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eed004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ancestries = ['EUR', 'AFR', 'AMR', 'EAS', 'SAS', 'AAC', 'AJ', 'CAH', 'CAS', 'MDE', 'FIN']\n",
    "output_dir = \"GP2/SORL1_haploview_and_det_imputed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for anc in ancestries:\n",
    "    file_path = os.path.join(output_dir, f\"{anc}_IM_RESULTS\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"✅ Processing {file_path} ...\")\n",
    "\n",
    "    data = []\n",
    "    current_block = None  \n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        header_line = f.readline().strip()\n",
    "        header_cols = header_line.split('\\t')\n",
    "        if header_cols[0].lower() == \"block\":\n",
    "            header_cols = header_cols[1:]  \n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"Block \"):  \n",
    "                current_block = line  \n",
    "                continue\n",
    "            fields = line.split('\\t')\n",
    "            if len(fields) == len(header_cols):\n",
    "                data.append([current_block] + fields)\n",
    "\n",
    "    if not data:\n",
    "        print(f\"⚠️ No data found inside {file_path}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    df_columns = ['Block'] + [\n",
    "        col.strip().replace(' ', '_').replace('.', '').replace(',', '') \n",
    "        for col in header_cols\n",
    "    ]\n",
    "    df = pd.DataFrame(data, columns=df_columns)\n",
    "\n",
    "    numeric_cols = ['Freq', 'Chi_Square', 'P_Value']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "\n",
    "    if 'CaseControl_Frequencies' in df.columns:\n",
    "        df[['CaseFreq', 'ControlFreq']] = df['CaseControl_Frequencies'] \\\n",
    "            .str.strip().str.split(',', expand=True).astype(float)\n",
    "\n",
    "    \n",
    "    cols_to_drop = ['Case_Control_Ratio_Counts', 'CaseControl_Frequencies']\n",
    "    for col in cols_to_drop:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "   \n",
    "    cols_to_keep = ['Block', 'Haplotype', 'Freq', 'Chi_Square', 'P_Value', 'CaseFreq', 'ControlFreq']\n",
    "    df = df[[col for col in cols_to_keep if col in df.columns]]\n",
    "\n",
    "   \n",
    "    output_path = os.path.join(output_dir, f\"{anc}_IM_cleaned.tsv\")\n",
    "    df.to_csv(output_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f86ac",
   "metadata": {},
   "source": [
    "### Seperate significant results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ancestries = ['EUR', 'AFR', 'AMR', 'EAS', 'SAS', 'AAC', 'AJ', 'CAH', 'CAS', 'MDE', 'FIN']\n",
    "input_dir = \"GP2/SORL1_haploview_and_det_imputed\"\n",
    "\n",
    "for anc in ancestries:\n",
    "    input_file = os.path.join(input_dir, f\"{anc}_IM_cleaned.tsv\")\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(input_file, sep='\\t')\n",
    "    \n",
    "    df['Direction'] = df.apply(\n",
    "        lambda row: '↑ in cases' if row['CaseFreq'] > row['ControlFreq']\n",
    "        else '↑ in controls' if row['CaseFreq'] < row['ControlFreq']\n",
    "        else 'no difference',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    sig_df = df[df['P_Value'] < 0.05].copy()\n",
    "    sig_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    output_file = os.path.join(input_dir, f\"{anc}_IM_significant.tsv\")\n",
    "    sig_df.to_csv(output_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31c0a4",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62397216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "ancestries = ['EUR', 'AFR', 'AMR', 'EAS', 'SAS', 'AAC', 'AJ', 'CAH', 'CAS', 'MDE', 'FIN']\n",
    "input_dir = \"GP2/SORL1_haploview_and_det_imputed\"\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "pdf_output = os.path.join(input_dir, \"all_IM_log2fc_heatmaps.pdf\")\n",
    "\n",
    "with PdfPages(pdf_output) as pdf:\n",
    "    for anc in ancestries:\n",
    "        input_file = os.path.join(input_dir, f\"{anc}_IM_significant.tsv\")\n",
    "        \n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"File not found: {input_file}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(input_file, sep='\\t')\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"No significant results for {anc.upper()}\")\n",
    "            continue\n",
    "\n",
    "        df['FoldChange'] = df['CaseFreq'] / df['ControlFreq']\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df['log2FC'] = np.log2(df['FoldChange'])\n",
    "\n",
    "        df = df.sort_values(by='FoldChange', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        output_table = os.path.join(input_dir, f\"{anc}_IM_significant_with_foldchange.tsv\")\n",
    "        df.to_csv(output_table, sep='\\t', index=False)\n",
    "        print(f\"Saved TSV: {output_table}\")\n",
    "\n",
    "        heatmap_df = df.set_index('Haplotype')[['log2FC']].copy()\n",
    "        finite_vals = heatmap_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        if finite_vals.dropna().empty and not any(np.isinf(heatmap_df['log2FC'])):\n",
    "            print(f\"Skipped heatmap for {anc.upper()} — nothing to plot\")\n",
    "            continue\n",
    "\n",
    "        mask = heatmap_df['log2FC'].isin([np.inf, -np.inf]).values.reshape(-1, 1)\n",
    "\n",
    "        max_hap_len = max(len(h) for h in heatmap_df.index)\n",
    "        fig_width = 10 if max_hap_len < 40 else 12 if max_hap_len < 80 else 14\n",
    "        fig_height = max(3.5, len(heatmap_df) * 0.45)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "        sns.heatmap(\n",
    "            finite_vals,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            cbar_kws={'label': 'log2(CaseFreq / ControlFreq)', 'shrink': 0.6},\n",
    "            mask=mask,\n",
    "            annot_kws={\"color\": \"black\"},\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        for i, (hap, val) in enumerate(heatmap_df['log2FC'].items()):\n",
    "            if val == np.inf or val == -np.inf:\n",
    "                color = 'red' if val == np.inf else 'blue'\n",
    "                ax.add_patch(plt.Rectangle((0, i), 1, 1, fill=True, color=color))\n",
    "                ax.text(0.5, i + 0.5, '∞', va='center', ha='center', color='black', fontsize=10, fontweight='bold')\n",
    "\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=6)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), fontsize=6)\n",
    "        ax.set_title(f'Log2 Fold Change (Case vs Control): {anc.upper()}', fontsize=10)\n",
    "        ax.set_ylabel('Haplotype', fontsize=8)\n",
    "        ax.set_xlabel(\"\")  \n",
    "\n",
    "        colorbar = ax.collections[0].colorbar\n",
    "        colorbar.ax.tick_params(labelsize=6)\n",
    "        colorbar.set_label('log2(CaseFreq / ControlFreq)', fontsize=7)\n",
    "\n",
    "        plt.subplots_adjust(left=0.42, right=0.92, top=0.88, bottom=0.08)\n",
    "\n",
    "        heatmap_image = os.path.join(input_dir, f\"{anc}_IM_log2fc_heatmap.png\")\n",
    "        plt.savefig(heatmap_image, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved PNG: {heatmap_image}\")\n",
    "\n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Added to PDF: {anc.upper()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b010f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "input_dir = \"GP2/SORL1_haploview_and_det_imputed\"\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "ordered_ancestries = ['EUR', 'AFR', 'AMR', 'AAC', 'AJ', 'CAS', 'EAS', 'SAS', 'MDE', 'CAH','FIN']\n",
    "\n",
    "pdf_output = os.path.join(input_dir, \"all_IM_case_enriched_heatmaps.pdf\")\n",
    "\n",
    "with PdfPages(pdf_output) as pdf:\n",
    "    for anc in ordered_ancestries:\n",
    "        input_file = os.path.join(input_dir, f\"{anc}_IM_significant.tsv\")\n",
    "\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"File not found: {input_file}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No significant results for {anc.upper()}\")\n",
    "            continue\n",
    "\n",
    "        df['FoldChange'] = df['CaseFreq'] / df['ControlFreq']\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df['log2FC'] = np.log2(df['FoldChange'])\n",
    "\n",
    "        df_case_enriched = df[df['log2FC'] > 0].copy()\n",
    "\n",
    "        if df_case_enriched.empty:\n",
    "            print(f\"No case-enriched haplotypes for {anc.upper()}\")\n",
    "            continue\n",
    "\n",
    "        df_case_enriched = df_case_enriched.sort_values(by='log2FC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        tsv_file = os.path.join(input_dir, f\"{anc}_IM_case_enriched.tsv\")\n",
    "        df_case_enriched.to_csv(tsv_file, sep='\\t', index=False)\n",
    "        print(f\"Saved TSV: {tsv_file}\")\n",
    "\n",
    "        heatmap_df = df_case_enriched.set_index('Haplotype')[['log2FC']].copy()\n",
    "        finite_vals = heatmap_df.replace([np.inf], np.nan)\n",
    "        mask = heatmap_df['log2FC'].isin([np.inf]).values.reshape(-1, 1)\n",
    "\n",
    "        max_hap_len = max(len(h) for h in heatmap_df.index)\n",
    "        fig_width = 10 if max_hap_len < 40 else 12 if max_hap_len < 80 else 14\n",
    "        fig_height = max(2, len(heatmap_df) * 0.5)\n",
    "\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        ax = sns.heatmap(\n",
    "            finite_vals,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap='Reds',\n",
    "            cbar_kws={'label': 'log2(CaseFreq / ControlFreq)'},\n",
    "            mask=mask,\n",
    "            annot_kws={\"color\": \"black\"}\n",
    "        )\n",
    "\n",
    "        for i, (hap, val) in enumerate(heatmap_df['log2FC'].items()):\n",
    "            if val == np.inf:\n",
    "                plt.gca().add_patch(plt.Rectangle((0, i), 1, 1, fill=True, color='darkred'))\n",
    "                plt.text(0.5, i + 0.5, '+inf', va='center', ha='center', color='black')\n",
    "\n",
    "        plt.yticks(rotation=0, fontsize=7)\n",
    "        plt.title(f'Case-Enriched Haplotypes: {anc.upper()}')\n",
    "        plt.ylabel('Haplotype')\n",
    "\n",
    "        if len(heatmap_df) < 4:\n",
    "            ax.figure.axes[-1].yaxis.label.set_size(8)\n",
    "            plt.xlabel('', fontsize=8)\n",
    "            plt.xticks(fontsize=8)\n",
    "        else:\n",
    "            plt.xlabel('')\n",
    "            plt.xticks(fontsize=10)\n",
    "\n",
    "        plt.subplots_adjust(left=0.45, right=0.95, top=0.85, bottom=0.10)\n",
    "\n",
    "        png_file = os.path.join(input_dir, f\"{anc}_IM_case_enriched_heatmap.png\")\n",
    "        plt.savefig(png_file)\n",
    "        print(f\"Saved PNG: {png_file}\")\n",
    "\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        print(f\"Added to PDF: {anc.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8c8c5-ffad-4b43-976b-1cecb3966adb",
   "metadata": {},
   "source": [
    "## Create Haplotype Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8781c-e8a0-4200-9480-479dc3c87153",
   "metadata": {},
   "outputs": [],
   "source": [
    "{WORK_DIR}/{ancestry}_boundaries_Hap.bim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496b6a3-cb9e-4828-a68e-41cde21a0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_boundaries_Hap \\\n",
    "    --maf 0.05 \\\n",
    "    --geno 0.05 \\\n",
    "    --make-bed \\\n",
    "    --out {WORK_DIR}/{ancestry}_filtered_Hap_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c397f7-bae8-4064-b90c-3505d267b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate the data by cases and controls\n",
    "\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_filtered_Hap_blocks \\\n",
    "    --make-bed \\\n",
    "    --filter-cases \\\n",
    "    --out {WORK_DIR}/{ancestry}_filtered_Hap_blocks_cases\n",
    "        \n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_filtered_Hap_blocks \\\n",
    "    --make-bed \\\n",
    "    --filter-controls \\\n",
    "    --out {WORK_DIR}/{ancestry}_filtered_Hap_blocks_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54e642f2-2230-4c33-a611-d6b4576acdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to vcf format before phasing\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    \n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_filtered_Hap_blocks_cases \\\n",
    "    --silent \\\n",
    "    --recode vcf-iid \\\n",
    "    --out {WORK_DIR}/{ancestry}_filtered_Hap_blocks_cases\n",
    "        \n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_filtered_Hap_blocks_controls \\\n",
    "    --silent \\\n",
    "    --recode vcf-iid \\\n",
    "    --out {WORK_DIR}/{ancestry}_filtered_Hap_blocks_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e22905-fd66-4208-9aa4-6128c645aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase data\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    \n",
    "    #cases\n",
    "    ! java -Xmx8g -jar /home/jupyter/{GENE}_results/haplotype_files/beagle.jar \\\n",
    "      gt={WORK_DIR}/{ancestry}_filtered_Hap_blocks_cases.vcf \\\n",
    "      out={WORK_DIR}/{ancestry}_filtered_Hap_blocks_cases_phased\n",
    "\n",
    "    #controls\n",
    "    ! java -Xmx8g -jar /home/jupyter/{GENE}_results/haplotype_files/beagle.jar \\\n",
    "      gt={WORK_DIR}/{ancestry}_filtered_Hap_blocks_controls.vcf \\\n",
    "      out={WORK_DIR}/{ancestry}_filtered_Hap_blocks_controls_phased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aa90a94-625f-4034-b18f-504b2c05c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert phased data vcfs to bfiles\n",
    "\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "\n",
    "    #cases\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --vcf {WORK_DIR}/{ancestry}_filtered_Hap_blocks_cases_phased.vcf.gz \\\n",
    "    --double-id \\\n",
    "    --pheno {WORK_DIR}/{ancestry}_pheno_doubleid.txt \\\n",
    "    --pheno-name PHENO \\\n",
    "    --allow-no-sex \\\n",
    "    --silent \\\n",
    "    --make-bed \\\n",
    "    --out {WORK_DIR}/{ancestry}_filtered_Hap_blocks_cases_phased\n",
    "        \n",
    "    # controls\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --vcf {WORK_DIR}/{ancestry}_filtered_Hap_blocks_controls_phased.vcf.gz \\\n",
    "    --double-id \\\n",
    "    --pheno {WORK_DIR}/{ancestry}_pheno_doubleid.txt \\\n",
    "    --pheno-name PHENO \\\n",
    "    --allow-no-sex \\\n",
    "    --silent \\\n",
    "    --make-bed \\\n",
    "    --out {WORK_DIR}/{ancestry}_filtered_Hap_blocks_controls_phased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29d295-345c-484f-bbb1-ccf71e4d5644",
   "metadata": {},
   "source": [
    "### Get haplotype blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f20a8a-348f-48d8-bd8a-351062155612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cases\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_filtered_Hap_blocks_cases_phased \\\n",
    "    --blocks no-pheno-req \\\n",
    "    --out {WORK_DIR}/{ancestry}_cases_phased_blocks_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e2911-60a7-41f2-bc80-81805fd01732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for controls\n",
    "for ancestry in ancestry_list:\n",
    "    CHROM = 'chr11'\n",
    "    GENE = 'SORL1'\n",
    "    WORK_DIR =  f'/home/jupyter/{GENE}_results/haplotype_files/{ancestry}_IMP'\n",
    "    ! /home/jupyter/tools/plink \\\n",
    "    --bfile {WORK_DIR}/{ancestry}_filtered_Hap_blocks_controls_phased \\\n",
    "    --blocks no-pheno-req \\\n",
    "    --out {WORK_DIR}/{ancestry}_controls_phased_blocks_imputed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
